lr: '2e-4' # learning rate
MAX_TRAIN_EPOCH: 30 # maximum number of training epoch
DEV_EPOCH: 1 # how many epoch during training we run a dev set
batch_size: 512 # batch size
N_W2V: 768 # word2vec dimension
hidden_size: 128 # for LSTM hidden states
task: 'kb_multihop' # kb_multihop, kb_completion, kb_lstm
use_cuda: True
gpu: '3' # gpu ids to train the model

DEBUG: False # the status of debug, debug flag

# MetaQA setting
meta_dir: '../data/MetaQA/'
kb_path: '../data/MetaQA/kb.txt' # path for knowledge base
n_hop: 2 # n hop for current model
embedding_method: 'BERT' # [BERT, Word2vec]
embedding_emb: '' # pre-trained embedding path for Word2vec
embedding_bert: '' # pre-trained BERT path for BERT
pool_size: 20 # multi-process pool size for Word2Vec
processed_query_dir: '../data/MetaQA/processed_query/'
entity_path: '../data/MetaQA/processed_query/entity_ids.txt' # entity-index mapping
relation_path: '../data/MetaQA/processed_query/relation_ids.txt' # relation-index mapping

1_hop_path: '../data/MetaQA/1-hop/vanilla/'
2_hop_path: '../data/MetaQA/2-hop/vanilla/'
3_hop_path: '../data/MetaQA/3-hop/vanilla/'
1_hop_output: '../data/MetaQA/embeddings_bert/1-hop/'
2_hop_output: '../data/MetaQA/embeddings_bert/2-hop/'
3_hop_output: '../data/MetaQA/embeddings_bert/3-hop/'
train_emb_path: '../data/MetaQA/embeddings_bert/3-hop/qa_train.txt' # extracted train question embedding path
dev_emb_path: '../data/MetaQA/embeddings_bert/3-hop/qa_dev.txt' # extracted dev question embedding path
test_emb_path: '../data/MetaQA/embeddings_bert/3-hop/qa_test.txt' # extracted test question embedding path

# log setting
logger_dir: '../log/' # log directory
tensorboard_path: '../tensorboard_log/' # tensorboard log directory
model_save_path: '../models/' # trained model save directory
model_name: 'bert003' # build a folder for each model based on name

# for inference
read_from: 'best_model' # best_model, final_model
